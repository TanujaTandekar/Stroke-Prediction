# -*- coding: utf-8 -*-
"""B20EE073_Prml_Course_Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nCgz_hY850GDL8cY53Si_bGSW0Cq0DQD
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from imblearn.over_sampling import SMOTE
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
import seaborn as sns 
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn import svm
from sklearn.neural_network import MLPClassifier
from sklearn.ensemble import GradientBoostingClassifier

from google.colab import drive
drive.mount('/content/drive')

#data=pd.read_csv('/content/drive/MyDrive/Colab Notebooks/healthcare-dataset-stroke-data.csv')
data=pd.read_csv('/content/healthcare-dataset-stroke-data1.csv')

"""#Preprocessing of the dataset"""

drop_features = ["id"]
data = data.drop(drop_features, axis=1)
data

data.shape

data.describe()

data.isna().sum()

#As we can observe in the above cell, bmi column have 201 missing values, thus to fill this missing values, we using knnImputer

from sklearn.impute import KNNImputer

imputer = KNNImputer(n_neighbors = 71)
data['bmi'] = imputer.fit_transform(data[['bmi']])

data.isna().sum()

data['stroke'].value_counts()[0]

data['stroke'].value_counts()[1]

fig, axes = plt.subplots(figsize=(6, 4))
data['stroke'].value_counts(normalize=True).plot.bar(width=0.2, color=('green','orange'))

plt.tight_layout()
plt.show()

df=data

df

df.shape

"""##Encoding"""

#encoding

from sklearn import preprocessing

label_encoder = preprocessing.LabelEncoder()
df['gender']= label_encoder.fit_transform(df['gender'])
df['ever_married']= label_encoder.fit_transform(df['ever_married'])
df['work_type']= label_encoder.fit_transform(df['work_type'])
df['Residence_type']= label_encoder.fit_transform(df['Residence_type'])
df['smoking_status']= label_encoder.fit_transform(df['smoking_status'])

df

"""##Spillting the dataset into train and test sets"""

from sklearn.model_selection import train_test_split

X = df.iloc[:, 0:-1].values
Y = df.iloc[:, -1].values

X_train, X_test, Y_train,Y_test = train_test_split(X, Y, test_size=.3, random_state=15)

Y

"""#Visualization"""

#visualizing the dataset

plt.figure(figsize=(7,7))
sns.distplot(data[data['stroke'] == 0]['age'], color='green')
sns.distplot(data[data['stroke'] == 1]['age'], color='red')
plt.title('Age: Stroke vs. No Stroke')

data['smoking_status'].unique()

# Smoking to Stroke
sns.catplot(y="smoking_status", hue="stroke", kind="count",palette="muted", edgecolor="0.6", data=data)
plt.show()

data['gender'].unique()

for feature in df.columns:
    plt.title(feature)
    sns.histplot(data = df , x = feature , hue = 'stroke')
    plt.show()

sns.pairplot(df, hue="stroke", size=3);
plt.show()

"""#As target column have classes imbalanced, thus in order to balance then we are using here smote"""

#using smote
from imblearn.over_sampling import SMOTE
sm = SMOTE(random_state = 2)
X_train1, Y_train1 = sm.fit_resample(X, Y.ravel())

"""#Training and testing the various models"""

import warnings
warnings.filterwarnings('ignore')

LGR = LogisticRegression(random_state=0)
LGR.fit(X_train1, Y_train1.ravel())
pred_LGR=LGR.predict(X_train1)
print('training acuracy for LogisticRegression model =',accuracy_score(Y_train1,pred_LGR))
print()
LGR1 = LogisticRegression(random_state=0)
LGR1.fit(X_train1, Y_train1.ravel())
pred_LGR1=LGR1.predict(X_test)
print('testing acuracy for LogisticRegression model =',accuracy_score(Y_test,pred_LGR1))

dtc = DecisionTreeClassifier(max_depth=22)
dtc.fit(X_train1,Y_train1)
pred11=dtc.predict(X_train1)
print(' training acuracy for DecisionTreeClassifier model',accuracy_score(Y_train1,pred11))
print()
dtc1 = DecisionTreeClassifier(max_depth=22)
dtc1.fit(X_train1,Y_train1.ravel())
pred112=dtc1.predict(X_test)
print(' testing acuracy for DecisionTreeClassifier model =',accuracy_score(Y_test,pred112))

lgbm = LGBMClassifier(n_estimators=2000, eta=0.5,max_depth=3)
lgbm.fit(X_train1,Y_train1.ravel())
pred4=lgbm.predict(X_train1)
print(' training acuracy for lightgbm model =',accuracy_score(Y_train1,pred4))
print()
lgbm1 = LGBMClassifier(n_estimators=2000, eta=0.5,max_depth=3)
lgbm1.fit(X_train1,Y_train1.ravel())
pred2=lgbm1.predict(X_test)
print(' testing acuracy for lightgbm model =',accuracy_score(Y_test,pred2))

rfc = RandomForestClassifier(n_estimators=700,max_depth=10)
rfc.fit(X_train1,Y_train1.ravel())
pred4=rfc.predict(X_train1)
print(' training acuracy for RandomForestClassifier =',accuracy_score(Y_train1,pred4))
print()
rfc1 = RandomForestClassifier(n_estimators=700,max_depth=10)
rfc1.fit(X_train1,Y_train1.ravel())
pred4=rfc1.predict(X_test)
print(' testing acuracy for RandomForestClassifier =',accuracy_score(Y_test,pred4))

knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(X_train1,Y_train1.ravel())
pred5=knn.predict(X_train1)
print(' testing acuracy for KNeighborsClassifier =',accuracy_score(Y_train1,pred5))
print()
knn1 = KNeighborsClassifier(n_neighbors=3)
knn1.fit(X_train1,Y_train1.ravel())
pred05=knn1.predict(X_test)
print(' testing acuracy for KNeighborsClassifier =',accuracy_score(Y_test,pred05))

Svc = svm.SVC()
Svc.fit(X_train1,Y_train1.ravel())
pred3=Svc.predict(X_train1)
print('acuracy for SVM =',accuracy_score(Y_train1,pred3))
print()
Svc1 = svm.SVC()
Svc1.fit(X_train1,Y_train1.ravel())
pred03=Svc1.predict(X_test)
print('acuracy for SVM =',accuracy_score(Y_test,pred03))

mlp = MLPClassifier(hidden_layer_sizes=(8,8,8), activation='relu', solver='adam', max_iter=5000)
mlp.fit(X_train1,Y_train1.ravel())
pred55=mlp.predict(X_train1)
print(' training acuracy for MLP =',accuracy_score(Y_train1,pred55))
print()
mlp1 = MLPClassifier(hidden_layer_sizes=(8,8,8), activation='relu', solver='adam', max_iter=5000)
mlp1.fit(X_train1,Y_train1.ravel())
pred66=mlp1.predict(X_test)
print(' testing acuracy for MLP =',accuracy_score(Y_test,pred66))

gdb = RandomForestClassifier(n_estimators=700,max_depth=10)
gdb.fit(X_train1,Y_train1.ravel())
pred77=gdb.predict(X_train1)
print(' training acuracy for Gradient Boosting =',accuracy_score(Y_train1,pred77))
print()
gdb1 = GradientBoostingClassifier(n_estimators=2000, learning_rate=0.1,max_depth=1, random_state=0)
gdb1.fit(X_train1,Y_train1.ravel())
pred88=gdb1.predict(X_test)
print(' testing acuracy for Gradient Boosting =',accuracy_score(Y_test,pred88))

# observing training and testing accuracies of various above models we can say that there is no overfit condition.

"""#Plotting ROC curve to compare the models"""

from sklearn.metrics import roc_auc_score, roc_curve

probs_LGR = LGR1.fit(X_train1, Y_train1.ravel()).predict_proba(X_test)[:, 1]
auc_LGR = roc_auc_score(Y_test, probs_LGR)
fpr_LGR, tpr_LGR, thresholds_LGR = roc_curve(Y_test, probs_LGR)

probs_dtc = dtc1.fit(X_train1, Y_train1.ravel()).predict_proba(X_test)[:, 1]
auc_dtc = roc_auc_score(Y_test, probs_dtc)
fpr_dtc, tpr_dtc, thresholds_dtc = roc_curve(Y_test, probs_dtc)

probs_lgbm = lgbm1.fit(X_train1, Y_train1.ravel()).predict_proba(X_test)[:, 1]
auc_lgbm = roc_auc_score(Y_test, probs_lgbm)
fpr_lgbm, tpr_lgbm, thresholds_lgbm = roc_curve(Y_test, probs_lgbm)

probs_rfc = rfc1.fit(X_train1, Y_train1.ravel()).predict_proba(X_test)[:, 1]
auc_rfc = roc_auc_score(Y_test, probs_rfc)
fpr_rfc, tpr_rfc, thresholds_rfc = roc_curve(Y_test, probs_rfc)

probs_knn = knn1.fit(X_train1, Y_train1.ravel()).predict_proba(X_test)[:, 1]
auc_knn = roc_auc_score(Y_test, probs_knn)
fpr_knn, tpr_knn, thresholds_knn = roc_curve(Y_test, probs_knn)

probs_mlp = mlp1.fit(X_train1, Y_train1.ravel()).predict_proba(X_test)[:, 1]
auc_mlp = roc_auc_score(Y_test, probs_mlp)
fpr_mlp, tpr_mlp, thresholds_mlp = roc_curve(Y_test, probs_mlp)

probs_gdb = gdb1.fit(X_train1, Y_train1.ravel()).predict_proba(X_test)[:, 1]
auc_gdb = roc_auc_score(Y_test, probs_gdb)
fpr_gdb, tpr_gdb, thresholds_gdb = roc_curve(Y_test, probs_gdb)

#probs_Svc = Svc1.fit(X_train1, Y_train1.ravel()).predict_proba(X_test)[:, 1]
#auc_Svc = roc_auc_score(Y_test, probs_Svc)
#fpr_Svc, tpr_Svc, thresholds_Svc = roc_curve(Y_test, probs_Svc)

plt.plot(fpr_LGR, tpr_LGR, label=f'AUC (Logistic Regression) = {auc_LGR:.2f}')
plt.plot(fpr_dtc, tpr_dtc, label=f'AUC (Decision Tree Classifier) = {auc_dtc:.2f}')
plt.plot(fpr_lgbm, tpr_lgbm, label=f'AUC (LGBMClassifier) = {auc_lgbm:.2f}')
plt.plot(fpr_rfc, tpr_rfc, label=f'AUC (RandomForestClassifier) = {auc_rfc:.2f}')
plt.plot(fpr_knn, tpr_knn, label=f'AUC (KNN) = {auc_knn:.2f}')
plt.plot(fpr_mlp, tpr_mlp, label=f'AUC (MLPClassifier) = {auc_mlp:.2f}')
plt.plot(fpr_gdb, tpr_gdb, label=f'AUC (GradientBoostingClassifier) = {auc_gdb:.2f}')
# plt.plot(fpr_Svc, tpr_Svc, label=f'AUC (RandomForestClassifier) = {auc_Svc:.2f}')
plt.title('ROC Curve')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend();

"""#Creating pipeline and testing the various models"""

from sklearn.pipeline import Pipeline

pipeline_lr=Pipeline([('scalar1',StandardScaler()),('lr_classifier',LogisticRegression(random_state=0))])

pipeline_dt=Pipeline([('scalar2',StandardScaler()),('dt_classifier',DecisionTreeClassifier(max_depth=22))])

pipeline_randomforest=Pipeline([('scalar3',StandardScaler()),('rf_classifier',RandomForestClassifier(n_estimators=700,max_depth=10))])

pipeline_lgbm=Pipeline([('scalar4',StandardScaler()),('lgbm_classifier',LGBMClassifier(n_estimators=2000, eta=0.5,max_depth=3))])

pipeline_knn=Pipeline([('scalar5',StandardScaler()),('knn_model',KNeighborsClassifier(n_neighbors=3))])

pipeline_svc=Pipeline([('scalar6',StandardScaler()),('svc_model',svm.SVC())])

pipeline_mlpClassifier=Pipeline([('scalar7',StandardScaler()),('mlp_regressor',MLPClassifier(hidden_layer_sizes=(8,8,8), activation='relu', solver='adam', max_iter=5000))])

pipeline_gradient_boosting_classsifier=Pipeline([('scalar8',StandardScaler()),('gradient_boosting_regressor',GradientBoostingClassifier(n_estimators=2000, learning_rate=0.1,max_depth=1, random_state=0))])

## Lets make the list of pipelines
pipelines = [pipeline_lr, pipeline_dt, pipeline_randomforest,pipeline_lgbm, pipeline_knn, pipeline_svc,pipeline_mlpClassifier,pipeline_gradient_boosting_classsifier]

best_accuracy=0.0
best_classifier=0
best_pipeline=""

# Dictionary of pipelines and classifier types for ease of reference
pipe_dict = {1: 'Logistic Regression', 2: 'Decision Tree', 3: 'RandomForest', 4: 'Lightgbm', 5: 'KNN', 6: 'SVM',7: 'MLP', 8: 'Gradient Boosting'}

# Fit the pipelines
for pipe in pipelines:
	pipe.fit(X_train1, Y_train1)

for i in pipe_dict:
  print(pipe_dict[i])

l1=[]
for i,model in zip(pipe_dict,pipelines):
  l1.append(model.score(X_test,Y_test))
  print("Test Accuracy:",pipe_dict[i],'=',model.score(X_test,Y_test))

plt.plot(l1[0],'--*',color='red')
plt.plot(l1[1],'--o',color='blue')
plt.plot(l1[2],'--X',color='green')
plt.plot(l1[3],'--*',color='pink')
plt.plot(l1[4],'--o',color='orange')
plt.plot(l1[5],'--X',color='purple')
plt.plot(l1[6],'--o',color='black')
plt.plot(l1[7],'--X',color='yellow')
plt.legend(['Logistic Regression','DecisionTreeClassifier','RandomForest','Lightgbm','KNeighborsClassifier','SVM','MLP','Gradient Boosting'])

for i,model in zip(pipe_dict,pipelines):
    if model.score(X_test,Y_test)>best_accuracy:
        best_accuracy=model.score(X_test,Y_test)
        best_pipeline=model
        best_classifier=i
print('Classifier with best accuracy : {}'.format(pipe_dict[best_classifier]),'-----> with accuracy = ',best_accuracy)